{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2caf47cf-c7d5-4635-a315-748b02ff2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import os.path\n",
    "\n",
    "sys.path.append(\"./training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11ff1b9-3492-49fb-8144-6dac2a426a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import worker_init_fn, get_pdbs, loader_pdb, build_training_clusters, PDB_dataset, StructureDataset, StructureLoader\n",
    "\n",
    "data_path = \"./pdb_2021aug02_sample\"\n",
    "params = {\n",
    "    \"LIST\"    : f\"{data_path}/list.csv\", \n",
    "    \"VAL\"     : f\"{data_path}/valid_clusters.txt\",\n",
    "    \"TEST\"    : f\"{data_path}/test_clusters.txt\",\n",
    "    \"DIR\"     : f\"{data_path}\",\n",
    "    \"DATCUT\"  : \"2030-Jan-01\",\n",
    "    \"RESCUT\"  : 3.5, #resolution cutoff for PDBs\n",
    "    \"HOMO\"    : 0.70 #min seq.id. to detect homo chains\n",
    "}\n",
    "\n",
    "LOAD_PARAM = {'batch_size': 1,\n",
    "              'shuffle': False,\n",
    "              'pin_memory':False,\n",
    "              'num_workers': 4}\n",
    "\n",
    "train, valid, test = build_training_clusters(params, False)\n",
    "\n",
    "train_set = PDB_dataset(list(train.keys()), loader_pdb, train, params)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23731fd-da05-4343-8477-12217c35b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_dict_train = get_pdbs(train_loader)\n",
    "dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=100000)\n",
    "loader_train = StructureLoader(dataset_train, batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573d358-b78f-4028-a175-71a7c605a3b0",
   "metadata": {},
   "source": [
    "### Mess around with architecture here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7e366c6-6f00-465e-88e1-c58141d9b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import ProteinFeatures, PositionWiseFeedForward, gather_nodes, cat_neighbors_nodes\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EncLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(EncLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.norm3 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W11 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W13 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, E_idx, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "        \n",
    "        # Compute node update.\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
    "        ## Duplicates the node embeddings up to K (nearest neighbors) for concatenation with edge embeddings.\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
    "        ## Compute message passing through linear layers in self.num_hidden embedding space.\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        \n",
    "        ## mask_V and mask_attend will not be None during training at least.\n",
    "        ## Zeros out the updates to the indices being ignored, before the update is applied to the node embeddings.\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "            \n",
    "        ## Compute the message by summing over the neighbor indices and dividing by a scale factor (should probably be the degree)\n",
    "        dh = torch.sum(h_message, -2) / self.scale\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "        \n",
    "        ## mask_V and mask_attend will not be None during training at least.\n",
    "        ## Zeros out the nodes being ignored, after the update is applied to the node embeddings (removes any layer biases added to the zeroed nodes).\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "\n",
    "        # Compute edge update.\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
    "        h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))\n",
    "        h_E = self.norm3(h_E + self.dropout3(h_message))\n",
    "        ## Looks like encoder block does not perform a 'dense layer' update for edge embeddings.\n",
    "        return h_V, h_E\n",
    "\n",
    "\n",
    "\n",
    "class DecLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(DecLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "        # The decoding layer works exactly like the encoding layer, but it does not perform an edge update. See line-by-line explanation from above.\n",
    "\n",
    "        # This block computes the node update.\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_E.size(-2),-1) \n",
    "        h_EV = torch.cat([h_V_expand, h_E], -1)\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / self.scale \n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "            \n",
    "        # Return updated node embeddings.\n",
    "        return h_V\n",
    "\n",
    "class ProteinMPNN(nn.Module):\n",
    "    def __init__(self, num_letters=21, node_features=128, edge_features=128,\n",
    "        hidden_dim=128, num_encoder_layers=3, num_decoder_layers=3,\n",
    "        vocab=21, k_neighbors=32, augment_eps=0.1, dropout=0.1):\n",
    "        super(ProteinMPNN, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.node_features = node_features\n",
    "        self.edge_features = edge_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.features = ProteinFeatures(node_features, edge_features, top_k=k_neighbors, augment_eps=augment_eps)\n",
    "\n",
    "        self.W_e = nn.Linear(edge_features, hidden_dim, bias=True)\n",
    "        self.W_s = nn.Embedding(vocab, hidden_dim)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncLayer(hidden_dim, hidden_dim*2, dropout=dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder layers\n",
    "        # The decoder layer hidden dim is 3x rather than 2x because \n",
    "        # encoder node embeddings are included in the edges\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecLayer(hidden_dim, hidden_dim*3, dropout=dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.W_out = nn.Linear(hidden_dim, num_letters, bias=True)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, X, S, mask, chain_M, residue_idx, chain_encoding_all):\n",
    "        \"\"\" Graph-conditioned sequence model \"\"\"\n",
    "        device=X.device\n",
    "        \n",
    "        # Prepare node and edge embeddings\n",
    "        # Constructs edge features from X.\n",
    "        # Computes virtual Cb atom position and encodes into RBF, adds additional edge features as needed.\n",
    "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
    "        \n",
    "        # Node features are intialized to zeros in the same hidden dimensionality as edges (128 by default)\n",
    "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device)\n",
    "        # Linear transformation over the edges to move them into the hidden dimensionality\n",
    "        h_E = self.W_e(E)\n",
    "\n",
    "        # Encoder is unmasked self-attention\n",
    "        # Duplicate the mask out to K (nearest neighbors) so that it can be used to mask which nearest neighbors will be used.\n",
    "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
    "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
    "        \n",
    "        # Perform the forward pass of the encoder layers with checkpointing.\n",
    "        # Computes a node and edge update which is used as input to the next layer and builds up the node representation.\n",
    "        for layer in self.encoder_layers:\n",
    "            h_V, h_E = torch.utils.checkpoint.checkpoint(layer, h_V, h_E, E_idx, mask, mask_attend)\n",
    "\n",
    "        # Concatenate sequence embeddings for autoregressive decoder\n",
    "        # Concatenates linear transformation of sequence (not one-hot encoded for some reason) to edge embeddings in place of node embeddings. \n",
    "        # Moves sequence from [B, N] -> [B, N, (node embedding) 128]\n",
    "        h_S = self.W_s(S)\n",
    "        h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)\n",
    "        \n",
    "        # (Justas comment) Build encoder embeddings \n",
    "        ## Concatenates zeros to edges in same shape as nodes would be concatenated to edges.\n",
    "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
    "        ## Concatenates node embeddings from encoder to edges.\n",
    "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
    "        \n",
    "        ## select only visible chains and only the residues that actually exist (not the padded residues added during batching) \n",
    "        ## element-wise multiplication of boolean vectors performs logical AND.\n",
    "        ## Update chain_M to include missing regions (Justas comment, this implies to me that mask also tracks gaps in structure/sequence)\n",
    "        chain_M = chain_M*mask \n",
    "        \n",
    "        # Apply auto-regressive masking\n",
    "        ## Creates a biased random shuffle of the indices in range [0,N), 0-indices in chain_M will be at the beginning of the shuffle while 1-indices will be at the end.\n",
    "        ## (Justas Comment) [numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
    "        decoding_order = torch.argsort((chain_M+0.0001)*(torch.abs(torch.randn(chain_M.shape, device=device)))) \n",
    "        \n",
    "        ## Creates a permutation matrix by constructing one-hot encodings for all of the indices in the biased random decoding order.\n",
    "        mask_size = E_idx.shape[1]\n",
    "        permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
    "        \n",
    "        ## Creates a random matrix for each protein that encodes the decoding order as a mask. Each row in the mask for each protein complex\n",
    "        ##   corresponds to which indices are visible for each step of the decoding process. Follows the \n",
    "        ##   For example:\n",
    "        ###    [0, 1, 0, 0, 0]\n",
    "        ###    [0, 0, 0, 0, 0]\n",
    "        ###    [1, 1, 0, 0, 0]\n",
    "        ###    [1, 1, 0, 1, 0]\n",
    "        ###    [1, 1, 0, 1, 1] for a 5 amino acid protein complex.\n",
    "        order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
    "        print(order_mask_backward.shape)\n",
    "        print(order_mask_backward[0])\n",
    "        \n",
    "        ## This creates a mask for which neighbors are visible for each index in the random decoding order.\n",
    "        ###  since each row in the order_mask_backward corresponds to a different index in the protein complex during auto-regressive decoding\n",
    "        ###  each index has its K nearest neighbors stored in the E_idx matrix. \n",
    "        ###  This call of the gather function creates a mask to find visible neighbors for each connection in E_idx.\n",
    "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
    "        print(mask_attend.shape)\n",
    "        print(mask_attend[0])\n",
    "              \n",
    "        ## Convert the mask from [B, N] -> [B, N, 1, 1] moves data to deepest index in a compatible dimensionality with mask_attend.\n",
    "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
    "        \n",
    "        ## Construct a mask and an inverse mask. Masks[B, N, KNN k, 1]\n",
    "        mask_bw = mask_1D * mask_attend\n",
    "        mask_fw = mask_1D * (1. - mask_attend)\n",
    "        print(mask_bw.shape)\n",
    "        print(mask_bw[0])\n",
    "        \n",
    "        print(mask_fw.shape)\n",
    "        print(mask_fw[0])\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # Add sequence embedding where appropriate otherwise use zeros-like in its place.\n",
    "        ## Using the forward mask select the indices of the zeros concat to node embeddings concat to edges for fw matrix.\n",
    "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
    "        for layer in self.decoder_layers:\n",
    "            ## Construct a matrix using the node embeddings updated on each iteration merged concat to sequence embedding concat to edges.\n",
    "            h_ESV = cat_neighbors_nodes(h_V, h_ES, E_idx)\n",
    "            ## Use the inverse mask to select only the visible indices of the seq embedding concat to node embedding concat to the edges \n",
    "            ##   add forward mask so we have values for every index.\n",
    "            h_ESV = mask_bw * h_ESV + h_EXV_encoder_fw\n",
    "            ## Compute node update with masked indices.\n",
    "            h_V = torch.utils.checkpoint.checkpoint(layer, h_V, h_ESV, mask)\n",
    "\n",
    "        # Linear map of logits to output dimensionality, softmax to prepare logits for NLL loss and return probs.\n",
    "        logits = self.W_out(h_V)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d427da-4d2e-40c7-9d35-c4bda703c201",
   "metadata": {},
   "source": [
    "### Run the messed-with model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d597816-2008-433d-9745-42ef6bb5983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import featurize\n",
    "\n",
    "model = ProteinMPNN(\n",
    "    node_features=128, \n",
    "    edge_features=128, \n",
    "    hidden_dim=128, \n",
    "    num_encoder_layers=3, \n",
    "    num_decoder_layers=3, \n",
    "    k_neighbors=3, # Set this to something small for simplicity during analysis.\n",
    "    # k_neighbors=48, \n",
    "    dropout=0.1, \n",
    "    augment_eps=0.2\n",
    ")\n",
    "\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa4af89d-d113-495f-a80b-6736b13cb79b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein Lengths:\n",
      "[4101] \n",
      "\n",
      "torch.Size([1, 4101, 4101])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([1, 4101, 3, 1])\n",
      "tensor([[[1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]]])\n",
      "torch.Size([1, 4101, 3, 1])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]]])\n",
      "torch.Size([1, 4101, 3, 1])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.],\n",
      "         [1.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.]]])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m mask_for_loss \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m*\u001b[39m chain_M\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass (is sequence aware!!), this is what we want to replicate for our model.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_M\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_encoding_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/pyrosetta/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mProteinMPNN.forward\u001b[0;34m(self, X, S, mask, chain_M, residue_idx, chain_encoding_all)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask_fw\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask_fw[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Add sequence embedding where appropriate otherwise use zeros-like in its place.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m## Using the forward mask select the indices of the zeros concat to node embeddings concat to edges for fw matrix.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m h_EXV_encoder_fw \u001b[38;5;241m=\u001b[39m mask_fw \u001b[38;5;241m*\u001b[39m h_EXV_encoder\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(loader_train):\n",
    "    print(\"Protein Lengths:\")\n",
    "    print([len(p['seq']) for p in batch], '\\n')\n",
    "    \n",
    "    # chain_M is the mask corresponding to \n",
    "    X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, torch.device('cpu'))\n",
    "    \n",
    "    # Multiplies the masks element-wise for a logical AND of the two masks, this is also performed in the forward pass.\n",
    "    # This has the effect of selecting only visible chains and only the residues that actually exist (not the padded residues added during batching).\n",
    "    mask_for_loss = mask * chain_M\n",
    "    \n",
    "    # Forward pass (is sequence aware!!), this is what we want to replicate for our model.\n",
    "    log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb84f3f6-433a-4be3-a188-d6689ec26432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protein_mpnn_utils import ProteinMPNN\n",
    "\n",
    "model = ProteinMPNN(\n",
    "    num_letters=20,\n",
    "    node_features=128, \n",
    "    edge_features=128, \n",
    "    hidden_dim=128, \n",
    "    num_encoder_layers=3, \n",
    "    num_decoder_layers=3, \n",
    "    k_neighbors=3, # Set this to something small for simplicity during analysis.\n",
    "    # k_neighbors=48, \n",
    "    dropout=0.1, \n",
    "    augment_eps=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4d00e-85d2-42ef-bc16-6ffac42ead07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
